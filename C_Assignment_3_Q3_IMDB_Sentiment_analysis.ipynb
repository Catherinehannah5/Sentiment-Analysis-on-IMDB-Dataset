{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "C  Assignment_3_Q3_IMDB_Sentiment_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6Ob7chMgjmL"
      },
      "source": [
        "##**Probelm-3 Sentiment Analysis using ImDb Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RPy55ovgpa-"
      },
      "source": [
        "####**Importing the Imdb dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5ahuFFknBdD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e269777e-0ba2-47bb-f6b2-67996adab0a4"
      },
      "source": [
        "#Importing the dataset\n",
        "import os\n",
        "import nltk \n",
        "import urllib.request as req\n",
        "import tarfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords') \n",
        "from nltk.corpus import stopwords \n",
        "\n",
        "from nltk.stem.porter import PorterStemmer \n",
        "\n",
        "#Downloading the ImDb dataset from the given link directly:\n",
        "imdb_url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "\n",
        "aclImdb_file = \"aclImdb_v1.tar.gz\"\n",
        "if not os.path.exists(aclImdb_file):\n",
        "    req.urlretrieve(imdb_url, aclImdb_file)\n",
        "    \n",
        "unzip_folder = \"aclImdb\"\n",
        "if not os.path.exists(unzip_folder):\n",
        "    with tarfile.open(aclImdb_file) as tar:\n",
        "        tar.extractall()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gr6A9eUm-HKu"
      },
      "source": [
        "def get_reviews(data_folder=\"/train\"):\n",
        "    reviews = []\n",
        "    labels = []\n",
        "    ps = PorterStemmer()\n",
        "    for index,sentiment in enumerate([\"/neg/\", \"/pos/\"]):\n",
        "        path = unzip_folder + data_folder + sentiment\n",
        "        for filename in sorted(os.listdir(path)):\n",
        "            with open(path + filename, 'r') as f:\n",
        "                review = f.read()\n",
        "                review = review.lower()\n",
        "                review = review.replace(\"<br />\", \" \")\n",
        "                review = re.sub(r\"[^a-z ]\", \" \", review) \n",
        "                review = re.sub(r\" +\", \" \", review)\n",
        "                review = review.split(\" \")\n",
        "                #review = review.split()\n",
        "                review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "                #review = ' '.join(review)\n",
        "                reviews.append(review)\n",
        "                \n",
        "                label = [0, 0]\n",
        "                label[index] = 1\n",
        "                labels.append(label)\n",
        "    #print(reviews[0])            \n",
        "    return reviews, np.array(labels)\n",
        "\n",
        "def get_test_reviews(data_folder=\"/test\"):\n",
        "    test_reviews = []\n",
        "    test_labels = []\n",
        "    ps = PorterStemmer()\n",
        "    for index,sentiment in enumerate([\"/neg/\", \"/pos/\"]):\n",
        "        path = unzip_folder + data_folder + sentiment\n",
        "        for filename in sorted(os.listdir(path)):\n",
        "            with open(path + filename, 'r') as f:\n",
        "                test_review = f.read()\n",
        "                test_review = test_review.lower()\n",
        "                test_review = test_review.replace(\"<br />\", \" \")\n",
        "                test_review = re.sub(r\"[^a-z ]\", \" \", test_review) \n",
        "                test_review = re.sub(r\" +\", \" \", test_review)\n",
        "                test_review = test_review.split(\" \")\n",
        "                #review = review.split()\n",
        "                test_review = [ps.stem(word) for word in test_review if not word in set(stopwords.words('english'))]\n",
        "                #review = ' '.join(review)\n",
        "                test_reviews.append(test_review)\n",
        "                \n",
        "                test_label = [0, 0]\n",
        "                test_label[index] = 1\n",
        "                test_labels.append(test_label)\n",
        "    #print(reviews[0])            \n",
        "    return test_reviews, np.array(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZZ7GQJb-L79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "312a75ed-bf5a-4742-9392-fceaa2dc2e48"
      },
      "source": [
        "\n",
        "train_reviews, train_labels = get_reviews()\n",
        "print(len(train_reviews))\n",
        "print(len(train_labels))\n",
        "print(train_reviews[0])\n",
        "print(train_reviews[1])\n",
        "print(train_labels[0])\n",
        "print(train_labels[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000\n",
            "25000\n",
            "['stori', 'man', 'unnatur', 'feel', 'pig', 'start', 'open', 'scene', 'terrif', 'exampl', 'absurd', 'comedi', 'formal', 'orchestra', 'audienc', 'turn', 'insan', 'violent', 'mob', 'crazi', 'chant', 'singer', 'unfortun', 'stay', 'absurd', 'whole', 'time', 'gener', 'narr', 'eventu', 'make', 'put', 'even', 'era', 'turn', 'cryptic', 'dialogu', 'would', 'make', 'shakespear', 'seem', 'easi', 'third', 'grader', 'technic', 'level', 'better', 'might', 'think', 'good', 'cinematographi', 'futur', 'great', 'vilmo', 'zsigmond', 'futur', 'star', 'salli', 'kirkland', 'freder', 'forrest', 'seen', 'briefli', '']\n",
            "['airport', 'start', 'brand', 'new', 'luxuri', 'plane', 'load', 'valuabl', 'paint', 'belong', 'rich', 'businessman', 'philip', 'steven', 'jame', 'stewart', 'fli', 'bunch', 'vip', 'estat', 'prepar', 'open', 'public', 'museum', 'also', 'board', 'steven', 'daughter', 'juli', 'kathleen', 'quinlan', 'son', 'luxuri', 'jetlin', 'take', 'plan', 'mid', 'air', 'plane', 'hi', 'jack', 'co', 'pilot', 'chamber', 'robert', 'foxworth', 'two', 'accomplic', 'banker', 'mont', 'markham', 'wilson', 'michael', 'pataki', 'knock', 'passeng', 'crew', 'sleep', 'ga', 'plan', 'steal', 'valuabl', 'cargo', 'land', 'disus', 'plane', 'strip', 'isol', 'island', 'make', 'descent', 'chamber', 'almost', 'hit', 'oil', 'rig', 'ocean', 'lose', 'control', 'plane', 'send', 'crash', 'sea', 'sink', 'bottom', 'right', 'bang', 'middl', 'bermuda', 'triangl', 'air', 'short', 'suppli', 'water', 'leak', 'flown', 'mile', 'cours', 'problem', 'mount', 'survivor', 'await', 'help', 'time', 'fast', 'run', 'also', 'known', 'slightli', 'differ', 'tile', 'airport', 'second', 'sequel', 'smash', 'hit', 'disast', 'thriller', 'airport', 'direct', 'jerri', 'jameson', 'like', 'predecessor', 'say', 'airport', 'sort', 'forgotten', 'classic', 'entertain', 'although', 'necessarili', 'right', 'reason', 'three', 'airport', 'film', 'seen', 'far', 'actual', 'like', 'one', 'best', 'favourit', 'plot', 'three', 'nice', 'mid', 'air', 'hi', 'jack', 'crash', 'see', 'oil', 'rig', 'sink', 'mayb', 'maker', 'tri', 'cross', 'origin', 'airport', 'anoth', 'popular', 'disast', 'flick', 'period', 'poseidon', 'adventur', 'submerg', 'stay', 'end', 'stark', 'dilemma', 'face', 'trap', 'insid', 'either', 'suffoc', 'air', 'run', 'drown', 'flood', 'door', 'open', 'decent', 'idea', 'could', 'made', 'great', 'littl', 'disast', 'flick', 'bad', 'unsympathet', 'charact', 'dull', 'dialogu', 'letharg', 'set', 'piec', 'real', 'lack', 'danger', 'suspens', 'tension', 'mean', 'miss', 'opportun', 'rather', 'sluggish', 'plot', 'keep', 'one', 'entertain', 'odd', 'minut', 'much', 'happen', 'plane', 'sink', 'much', 'urgenc', 'thought', 'even', 'navi', 'becom', 'involv', 'thing', 'pick', 'much', 'shot', 'huge', 'ship', 'helicopt', 'fli', 'someth', 'lack', 'georg', 'kennedi', 'jinx', 'airlin', 'worker', 'joe', 'patroni', 'back', 'get', 'coupl', 'scene', 'bare', 'even', 'say', 'anyth', 'prefer', 'look', 'worri', 'background', 'home', 'video', 'theatric', 'version', 'airport', 'run', 'minut', 'us', 'tv', 'version', 'add', 'extra', 'hour', 'footag', 'includ', 'new', 'open', 'credit', 'sequenc', 'mani', 'scene', 'georg', 'kennedi', 'patroni', 'flashback', 'flesh', 'charact', 'longer', 'rescu', 'scene', 'discoveri', 'anoth', 'coupl', 'dead', 'bodi', 'includ', 'navig', 'would', 'like', 'see', 'extra', 'footag', 'sure', 'could', 'sit', 'near', 'three', 'hour', 'cut', 'airport', 'expect', 'film', 'date', 'badli', 'horribl', 'fashion', 'interior', 'design', 'choic', 'say', 'toy', 'plane', 'model', 'effect', 'great', 'either', 'along', 'two', 'airport', 'sequel', 'take', 'pride', 'place', 'razzi', 'award', 'hall', 'shame', 'although', 'think', 'lot', 'wors', 'film', 'reckon', 'littl', 'harsh', 'action', 'scene', 'littl', 'dull', 'unfortun', 'pace', 'slow', 'much', 'excit', 'tension', 'gener', 'shame', 'reckon', 'could', 'pretti', 'good', 'film', 'made', 'properli', 'product', 'valu', 'alright', 'noth', 'spectacular', 'act', 'great', 'two', 'time', 'oscar', 'winner', 'jack', 'lemmon', 'said', 'sinc', 'mistak', 'star', 'one', 'time', 'oscar', 'winner', 'jame', 'stewart', 'look', 'old', 'frail', 'also', 'one', 'time', 'oscar', 'winner', 'lee', 'grant', 'look', 'drunk', 'sir', 'christoph', 'lee', 'given', 'littl', 'plenti', 'familiar', 'face', 'look', 'airport', 'disast', 'orient', 'three', 'airport', 'film', 'far', 'like', 'idea', 'behind', 'even', 'bit', 'silli', 'product', 'bland', 'direct', 'help', 'though', 'film', 'sunken', 'plane', 'bore', 'letharg', 'follow', 'concord', 'airport', '']\n",
            "[1 0]\n",
            "[1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVINJxGvybku",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3864efde-e8f7-4f72-9ae7-47cca88c8c9b"
      },
      "source": [
        "test_reviews, test_labels = get_test_reviews()\n",
        "print(len(test_reviews))\n",
        "print(len(test_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000\n",
            "25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYvSmtE-rFZc"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "NB_WORDS = 10000  # Parameter indicating the number of words we'll put in the dictionary i.e ->most frequently used words\n",
        "tk = Tokenizer(num_words=NB_WORDS,\n",
        "filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{\"}~\\t\\n',lower=True, split=\" \")\n",
        "tk.fit_on_texts(train_reviews)\n",
        "X_train_seq = tk.texts_to_sequences(train_reviews)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUhUaaF7rmUl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "82a12b2b-36d0-4c13-a31c-345953bb2ae4"
      },
      "source": [
        "print(X_train_seq[0])\n",
        "print(X_train_seq[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[13, 55, 5318, 62, 2748, 86, 246, 18, 1143, 355, 1266, 104, 6805, 5694, 177, 94, 1461, 958, 2502, 796, 6189, 1411, 352, 435, 1266, 143, 6, 256, 1157, 705, 8, 139, 14, 861, 94, 9493, 335, 15, 8, 1606, 39, 687, 711, 6371, 1025, 448, 58, 155, 30, 7, 565, 615, 26, 615, 76, 2867, 8582, 5380, 47, 2868, 4]\n",
            "[3653, 86, 2706, 80, 4775, 1327, 1543, 3613, 974, 1663, 869, 4037, 2378, 1472, 502, 1103, 802, 675, 2855, 1371, 246, 826, 3458, 27, 1482, 1472, 418, 1966, 5566, 374, 4775, 48, 655, 1520, 661, 1327, 4921, 583, 850, 1392, 5079, 491, 42, 7858, 9494, 2169, 402, 1557, 3704, 918, 945, 2105, 655, 897, 3613, 8791, 911, 1327, 2070, 2513, 901, 8, 3791, 5079, 138, 378, 2674, 6908, 3107, 668, 797, 1327, 1177, 1383, 1639, 2287, 1180, 112, 2644, 570, 4513, 661, 257, 3044, 790, 7024, 1318, 188, 202, 4514, 2308, 3285, 162, 6, 622, 173, 27, 496, 988, 148, 3653, 206, 525, 2911, 378, 1400, 521, 3653, 96, 1320, 7025, 5, 3441, 38, 3653, 326, 1408, 237, 161, 186, 2365, 112, 133, 209, 3653, 2, 47, 150, 68, 5, 3, 53, 1338, 41, 209, 207, 1520, 661, 4921, 583, 1383, 11, 2674, 6908, 2287, 201, 840, 54, 1060, 89, 3653, 79, 853, 1400, 338, 658, 713, 435, 22, 4149, 4431, 212, 1301, 875, 294, 8583, 661, 173, 2842, 4975, 961, 246, 460, 178, 36, 34, 26, 52, 1400, 338, 24, 5260, 9, 665, 335, 8792, 87, 264, 71, 302, 915, 544, 932, 151, 238, 1081, 171, 9495, 41, 189, 3, 161, 792, 100, 21, 108, 1327, 2287, 21, 6806, 98, 14, 2535, 127, 265, 35, 497, 21, 149, 548, 1164, 3045, 802, 67, 302, 594, 3286, 6687, 1621, 804, 64, 10, 286, 18, 877, 14, 38, 152, 1353, 19, 1483, 770, 268, 293, 1784, 196, 3653, 173, 100, 97, 166, 196, 471, 981, 236, 842, 258, 80, 246, 421, 340, 46, 18, 594, 3286, 1234, 1534, 9, 1094, 1466, 18, 2967, 79, 286, 289, 452, 258, 8584, 15, 5, 11, 981, 842, 141, 36, 432, 659, 209, 236, 369, 3653, 146, 2, 635, 828, 392, 971, 3881, 744, 795, 38, 1652, 1327, 1269, 111, 26, 294, 310, 42, 3653, 525, 48, 2574, 144, 7714, 778, 1875, 747, 186, 30, 70, 383, 2, 6190, 52, 2060, 103, 18, 52, 665, 352, 467, 449, 21, 553, 932, 256, 747, 6190, 36, 105, 7, 2, 34, 2487, 214, 549, 2320, 82, 1842, 32, 26, 42, 6, 545, 1739, 583, 3287, 230, 157, 862, 76, 3, 6, 545, 1739, 502, 1103, 19, 72, 8391, 27, 3, 6, 545, 1739, 677, 1082, 19, 1544, 2298, 1209, 677, 299, 52, 846, 939, 212, 19, 3653, 1400, 2774, 209, 3653, 2, 150, 5, 178, 429, 14, 123, 573, 214, 1473, 96, 162, 74, 2, 1327, 184, 8792, 210, 3653, 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CamFpP0Cr2He",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "1dee0633-43a2-4439-c754-97c45d8e2431"
      },
      "source": [
        "VAL_SIZE = 1000  # Size of the validation set\n",
        "NB_START_EPOCHS = 10  # Number of epochs we usually start to train with\n",
        "BATCH_SIZE = 512  # Size of the batches used in the mini-batch gradient descent\n",
        "MAX_LEN = 100  # Maximum number of words in a sequence\n",
        "GLOVE_DIM = 100  # Number of dimensions of the GloVe word embeddings\n",
        "\n",
        "max_features = 10000\n",
        "maxlen = 100\n",
        "batch_size = 64\n",
        "embedding_dims = 16\n",
        "filters = 128 # was 128\n",
        "kernel_size = 3\n",
        "epochs = 3\n",
        "#Equal length of sequences:\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "X_train_seq_trunc = pad_sequences(X_train_seq, maxlen=MAX_LEN)\n",
        "print(X_train_seq_trunc[0])\n",
        "print(X_train_seq_trunc[10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0   13   55 5318\n",
            "   62 2748   86  246   18 1143  355 1266  104 6805 5694  177   94 1461\n",
            "  958 2502  796 6189 1411  352  435 1266  143    6  256 1157  705    8\n",
            "  139   14  861   94 9493  335   15    8 1606   39  687  711 6371 1025\n",
            "  448   58  155   30    7  565  615   26  615   76 2867 8582 5380   47\n",
            " 2868    4]\n",
            "[  59 3615   92 7148 1502 4039  172  786    1 1190   33  274 1198  147\n",
            " 1520 2440  786 3288  877  539   24  116  339 5080  824 1125 2277   99\n",
            "   71   49  885  477  331  946    3  952  147    7  133   73  199  260\n",
            " 3418   30   13  757   96  287   40   60  175    1   24  158   22 2322\n",
            "  167    2  329  209  375  348 2408 4822 3821 8203  913 1822  209  367\n",
            "    8    1 3123   99  209  220 2277 1301   16   24    2    3   36   56\n",
            "    7  118 1118 1939  119  834    5  209  348   12   67  260    1  216\n",
            "    6    4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACv8PYCSs_4_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "30210cd0-3afd-4f32-c6b8-4f243b695103"
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Conv1D, GlobalMaxPool1D, Flatten, MaxPool1D, Dropout\n",
        "'''\n",
        "emb_model = models.Sequential()\n",
        "emb_model.add(layers.Embedding(NB_WORDS, 8, input_length=MAX_LEN))\n",
        "emb_model.add(layers.Flatten())\n",
        "emb_model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "max_features = 5000\n",
        "maxlen = 100\n",
        "batch_size = 64\n",
        "embedding_dims = 16\n",
        "filters = 128\n",
        "kernel_size = 3\n",
        "epochs = 3\n",
        "\n",
        "\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "'''\n",
        "epochs = 5\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
        "model.add(Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1))\n",
        "model.add(Conv1D(64, 3, border_mode='same'))\n",
        "model.add(Conv1D(32, 3, border_mode='same'))\n",
        "model.add(Conv1D(16, 3, border_mode='same'))\n",
        "model.add(GlobalMaxPool1D())\n",
        "#model.add(MaxPool1D())\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(128, activation='sigmoid'))\n",
        "#model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(2, activation='sigmoid'))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(64, 3, padding=\"same\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(32, 3, padding=\"same\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(16, 3, padding=\"same\")`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hy2Bk-FZEO6_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "94b7f18d-b99e-4ead-bd81-d7b248db0e8e"
      },
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model = model.save('/content/20868337_NLP_model')\n",
        "print(X_train_seq_trunc.shape)\n",
        "print(train_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000, 100)\n",
            "[[1 0]\n",
            " [1 0]\n",
            " [1 0]\n",
            " ...\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJd-7Pq6EVn7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "7327e4fa-3483-4516-954e-9209ddf76dfb"
      },
      "source": [
        "from tensorflow import keras\n",
        "saved_model = keras.models.load_model('/content/20868337_NLP_model')\n",
        "saved_model.fit(X_train_seq_trunc, train_labels, batch_size=batch_size, epochs=epochs)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "391/391 [==============================] - 23s 59ms/step - loss: 0.5233 - accuracy: 0.7178\n",
            "Epoch 2/5\n",
            "391/391 [==============================] - 23s 59ms/step - loss: 0.3377 - accuracy: 0.8561\n",
            "Epoch 3/5\n",
            "391/391 [==============================] - 23s 59ms/step - loss: 0.2421 - accuracy: 0.9062\n",
            "Epoch 4/5\n",
            "391/391 [==============================] - 23s 59ms/step - loss: 0.1671 - accuracy: 0.9384\n",
            "Epoch 5/5\n",
            "391/391 [==============================] - 23s 59ms/step - loss: 0.1159 - accuracy: 0.9592\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1450358128>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfqemdIvZvD9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "outputId": "d232d4c1-a11e-4c41-ba5c-7e749df6cdab"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "NB_WORDS = 10000  # Parameter indicating the number of words we'll put in the dictionary i.e ->most frequently used words\n",
        "tk = Tokenizer(num_words=NB_WORDS,\n",
        "filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{\"}~\\t\\n',lower=True, split=\" \")\n",
        "tk.fit_on_texts(test_reviews)\n",
        "X_test_seq = tk.texts_to_sequences(test_reviews)\n",
        "\n",
        "print(X_test_seq[0])\n",
        "print(X_test_seq[1])\n",
        "\n",
        "VAL_SIZE = 1000  # Size of the validation set\n",
        "NB_START_EPOCHS = 10  # Number of epochs we usually start to train with\n",
        "BATCH_SIZE = 512  # Size of the batches used in the mini-batch gradient descent\n",
        "MAX_LEN = 100  # Maximum number of words in a sequence\n",
        "GLOVE_DIM = 100  # Number of dimensions of the GloVe word embeddings\n",
        "\n",
        "max_features = 10000\n",
        "maxlen = 100\n",
        "batch_size = 64\n",
        "embedding_dims = 16\n",
        "filters = 128\n",
        "kernel_size = 3\n",
        "epochs = 3\n",
        "#Equal length of sequences:\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "X_test_seq_trunc = pad_sequences(X_test_seq, maxlen=MAX_LEN)\n",
        "print(X_test_seq_trunc[0])\n",
        "print(X_test_seq_trunc[10])\n",
        "\n",
        "\n",
        "from tensorflow import keras\n",
        "reconstructed_NLP_model = keras.models.load_model('/content/20868337_NLP_model')\n",
        "score = reconstructed_NLP_model.evaluate(X_test_seq_trunc, test_labels, batch_size=32, verbose=1)\n",
        "\n",
        "print(reconstructed_NLP_model.metrics_names)\n",
        "print(score)\n",
        "print(\"NLP model - Testing loss = {:2f}\".format(score[0]))\n",
        "print(\"NLP model - Testing accuracy = {:2f}\".format(score[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[313, 4522, 946, 1, 141, 1009, 1627, 1034, 1221, 1185, 1365, 349, 243, 9, 104, 1073, 3246, 4522, 9, 438, 352, 1372, 21, 245, 6, 243, 9, 16, 243, 6863, 6266, 5406, 213, 47, 132, 31, 57, 163, 257, 107, 19, 1133, 8813, 3246, 5691, 190, 623, 4522, 137, 18, 437, 216, 33, 90, 2797, 4522, 140, 104, 5406, 1073, 490, 5406, 1866, 52, 2213, 5771, 923, 37, 184, 95, 234, 4]\n",
            "[364, 450, 108, 2, 250, 185, 16, 81, 195, 11, 134, 229, 825, 3394, 297, 1488, 1488, 9054, 7871, 41, 6, 1723, 29, 29, 18, 705, 3, 27, 10, 93, 567, 431, 11, 93, 878, 4033, 1488, 1328, 1962, 1946, 395, 1975, 1488, 9054, 10, 72, 389, 1488, 346, 470, 327, 585, 8, 2, 3372, 2315, 60, 111, 1811, 695, 4997, 2, 1811, 136, 30, 2136, 9, 542, 8585, 84, 2, 44, 695, 4997, 30, 2136, 1347, 9, 145, 542, 9, 237, 35, 630, 373, 221, 217, 108, 998, 3276, 57, 2, 10, 16, 46, 10, 3, 11, 5259, 212, 1253, 7550, 620, 57, 29, 57, 126, 34, 35, 195, 11, 462, 241, 269, 565, 133, 2137, 47, 325, 8, 346, 2, 100, 4]\n",
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "  313 4522  946    1  141 1009 1627 1034 1221 1185 1365  349  243    9\n",
            "  104 1073 3246 4522    9  438  352 1372   21  245    6  243    9   16\n",
            "  243 6863 6266 5406  213   47  132   31   57  163  257  107   19 1133\n",
            " 8813 3246 5691  190  623 4522  137   18  437  216   33   90 2797 4522\n",
            "  140  104 5406 1073  490 5406 1866   52 2213 5771  923   37  184   95\n",
            "  234    4]\n",
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0 2381   30 1769   12  220  486 1575 1932   67   24\n",
            "   44 6268 3999  260 6748 1640  304  957 3531   11    2  193  153    2\n",
            " 1761 3531 1867 2381   30   63   16  570    5   25   43 1867 2381  439\n",
            "   82   25   43 1811 1071   37   57  938  168 1867 3494 3173  212  518\n",
            " 1299    4]\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.8428 - accuracy: 0.5000\n",
            "['loss', 'accuracy']\n",
            "[0.8427572846412659, 0.5]\n",
            "NLP model - Testing loss = 0.842757\n",
            "NLP model - Testing accuracy = 50.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQ6WwsdwH_-i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "b19ab660-04ae-4b5c-82cf-6739b27b8429"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from keras.datasets import imdb\n",
        "\n",
        "(X_train,y_train), (X_test,y_test) = tf.keras.datasets.imdb.load_data(\n",
        "    path='imdb.npz', num_words=None, skip_top=0, maxlen=None, seed=113,\n",
        "    start_char=1, oov_char=2, index_from=3)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "(25000,)\n",
            "(25000,)\n",
            "(25000,)\n",
            "(25000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4fFLhJdtIpN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "655cbde5-3f68-4215-c4fa-e3a3b5417d48"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Conv1D, GlobalMaxPool1D\n",
        "from keras.datasets import imdb\n",
        "\n",
        "max_features = 5000\n",
        "maxlen = 100\n",
        "batch_size = 64\n",
        "embedding_dims = 16\n",
        "filters = 128\n",
        "kernel_size = 3\n",
        "epochs = 3\n",
        "\n",
        "(x_train, y_train), (_, _) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
        "model.add(Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1))\n",
        "model.add(GlobalMaxPool1D())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "25000/25000 [==============================] - 7s 267us/step - loss: 0.4887 - accuracy: 0.7406\n",
            "Epoch 2/3\n",
            "25000/25000 [==============================] - 6s 240us/step - loss: 0.2945 - accuracy: 0.8774\n",
            "Epoch 3/3\n",
            "25000/25000 [==============================] - 6s 238us/step - loss: 0.2174 - accuracy: 0.9134\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f2129347550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    }
  ]
}